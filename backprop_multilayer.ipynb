{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backpropagation in Multilayer Neural Networks\n",
    "\n",
    "### Goals: \n",
    "- implementING a real gradient descent in `Numpy`\n",
    "\n",
    "### Dataset:\n",
    "- Similar as first Lab - Digits: 10 class handwritten digits\n",
    "- [sklearn.datasets.load_digits](http://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_digits.html#sklearn.datasets.load_digits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_digits\n",
    "\n",
    "digits = load_digits()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAMIAAADSCAYAAAAYNx+qAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAADMxJREFUeJzt3XuMXGUdxvHvw3IrUFqhhUAXGCKUgCZstWIIYio3i1SKxkRAMdtoMBqEDRqgJuLqH14Sg63GiFrQxiJIqdZLEITQRpsI0pYqlIIpZbGtyG4jtYWoUPj5xzmbTJfd7rvlXGZ2n08y6cycM+/57XaePdf3vIoIzCa6A+ouwKwVOAhmOAhmgINgBjgIZoCDYAZMwCBI2ihpTt117IukbklrEuftlbRsP5ez358dbyZcECLibRGxuu462o2kMyStlfRi/nhQ0hl111WUCRcE22//AD4CHAVMA34N3FVrRQWacEGQ1Cfpgvx5r6TlkpZJ2i3pcUkzJS2U1C9pq6SLmj67QNKmfN4tkj49pO0bJD0v6R+SPiUpJJ2STztE0rck/V3SC5JulTQpsebFeS27JK2TdO6QWQ6V9PO8rvWSzmz67PGSVkgakPSspGv35/cWETsjoi+ySxEEvAacsj9ttaIJF4RhfBD4KfAW4DHgfrLfywzgq8APmubtB+YBRwILgG9LegeApLnA9cAFZF+QOUOW8w1gJtCVT58B3JxY46P5544CfgYsl3Ro0/T5wPKm6SslHSTpAOA3wF/y5Z0P9Eh6/3ALkfRXSVfuqxBJO4H/At8FvpZYf+uLiAn1APqAC/LnvcADTdM+CLwEdOSvJwMBTB2hrZXAdfnz24GvN007Jf/sKWR/QV8G3to0/Wzg2RHa7QbW7ONneBE4s+lneLhp2gHA88C5wLuBvw/57ELgx02fXbYfv8PDgc8Cl9T9/1nU48Bi4tTWXmh6/h9gR0S81vQa4Ahgp6SLgS+T/WU/ADgMeDyf53hgbVNbW5ueT8/nXSdp8D0BHSkFSvoC8Ml8GUG2Rpo23LIi4nVJ25rmPT7/Kz6oA/hjynJHEhEvS7oVGJB0ekT0v5n2WoGDkEjSIcAK4BPAryLiVUkryb7QkP0V7mz6yAlNz3eQheptEbF9jMs9F7iBbLNmY/5Ff7FpuXstK98c6iTbud1DttY5dSzLTDT4h2AG2SZjW/M+QrqDgUOAAWBPvna4qGn63cACSadLOgz40uCEiHgd+BHZPsUxAJJmjLStPsRksi/0AHCgpJvJ1gjN3inpw5IOBHqA/wEPA38Gdku6UdIkSR2S3i7pXWP94SVdKGlW3saRwC1km2ibxtpWK3IQEkXEbuBasi/8i8CVZIcQB6f/DvgOsArYTPZFhOxLCXDj4PuSdgEPAqclLPp+4D7gb8BzZDuqW4fM8yvgo3ldVwEfjohX8028eWQ72s+SrZmWAFOGW1B+svFjI9QxFbgT+DfwDPBWYG5E/DfhZ2h5ynd+rGCSTgeeAA6JiD1112P75jVCgSR9KD9f8Bbgm8BvHIL24CAU69NkO47PkJ1w+ky95VgqbxqZ4TWCGeAgmAElnVCbNm1aNBqNMpouzNatQ49Avjn9/cWfU5o0KemavGTHHntsoe0BHH300YW3WaS+vj527Nih0eYrJQiNRoO1a9eOPmONenp6Cm1v8eLFhbYHMHPmzELbK/pnBuju7i68zSLNnj07aT5vGpnhIJgBDoIZ4CCYAYlBkDRX0tOSNku6qeyizKo2ahAkdQDfAy4GzgCuGE93LzCDtDXCWcDmiNgSEa+Q3blgfrllmVUrJQgz2Pv69235e2bjRmE7y5Kuzm8AtXZgYKCoZs0qkRKE7ezd/7Yzf28vEfHDiJgdEbOnT59eVH1mlUgJwqPAqZJOlnQwcDlNXRTNxoNRrzWKiD2SriHrO9sB3B4RG0uvzKxCSRfdRcS9wL0l12JWG59ZNsNBMAMcBDPAQTADJvC9T7u6ugptb+XKlYW2B3DZZZcV2t6CBQsKbQ9av4daKq8RzHAQzAAHwQxwEMwAB8EMcBDMAAfBDEjrs3x7PubwE1UUZFaHlDXCT4C5JddhVqtRgxARfwD+VUEtZrVxn2UzCgyC+yxbO/NRIzMcBDMg7fDpncCfgNMkbZP0yfLLMqtWyl0srqiiELM6edPIDAfBDHAQzAAHwQyYwJ33i+503tvbW2h7AFOmTCm0vaVLlxba3njiNYIZDoIZ4CCYAQ6CGeAgmAEOghmQdtHdCZJWSXpS0kZJ11VRmFmVUs4j7AE+HxHrJU0G1kl6ICKeLLk2s8qk9Fl+PiLW5893A5vwOMs2zoxpH0FSA5gFPFJGMWZ1SQ6CpCOAFUBPROwaZro771vbSgqCpIPIQnBHRPxiuHnced/aWcpRIwG3AZsi4pbySzKrXsoa4RzgKuA8SRvyxwdKrsusUil9ltcAqqAWs9r4zLIZDoIZ4CCYAQ6CGTCB+ywXbdasWYW3OXXq1ELbO+mkkwptbzzxGsEMB8EMcBDMAAfBDHAQzAAHwQxwEMyAtMuwD5X0Z0l/yTvvf6WKwsyqlHJC7X/AeRHxUt5BZ42k30XEwyXXZlaZlMuwA3gpf3lQ/ogyizKrWmpXzQ5JG4B+4IGIeEPnffdZtnaWFISIeC0iuoBO4CxJbx9mHvdZtrY1pqNGEbETWAXMLaccs3qkHDWaLmlq/nwScCHwVNmFmVUp5ajRccBSSR1kwbk7In5bbllm1Uo5avRXsrvbmY1bPrNshoNgBjgIZoCDYAa4835h5s+fX3ibq1atKrS9OXPmFNoewIYNGwptr9FoFNpeKq8RzHAQzAAHwQxwEMwAB8EMcBDMgLENJtgh6TFJvuDOxp2xrBGuIxtj2WzcSe2q2QlcAiwptxyzeqSuERYBNwCvjzSD+yxbO0vpoTYP6I+Idfuaz32WrZ2lDi97qaQ+4C6yYWaXlVqVWcVGDUJELIyIzohoAJcDD0XEx0uvzKxCPo9gxhgvw46I1cDqUioxq5HXCGY4CGaAg2AGOAhmgPsst7RFixYV2l5fX1+h7QF0d3cX2t7q1asLbS+V1whmOAhmgINgBjgIZoCDYAY4CGZA4uHT/BLs3cBrwJ6ImF1mUWZVG8t5hPdFxI7SKjGrkTeNzEgPQgC/l7RO0tVlFmRWh9RNo/dExHZJxwAPSHoqIv7QPEMekKsBTjzxxILLNCtX6oDj2/N/+4FfAmcNM48771vbSrmLxeGSJg8+By4Cnii7MLMqpWwaHQv8UtLg/D+LiPtKrcqsYinjLG8BzqygFrPa+PCpGQ6CGeAgmAEOghngIJgBE7jzftGdxMvodF70YN5l1NjV1VV4m3XwGsEMB8EMcBDMAAfBDHAQzAAHwQxIH152qqR7JD0laZOks8suzKxKqecRFgP3RcRHJB0MHFZiTWaVGzUIkqYA7wW6ASLiFeCVcssyq1bKptHJwADwY0mPSVqS91Tbiwcct3aWEoQDgXcA34+IWcDLwE1DZ3KfZWtnKUHYBmyLiEfy1/eQBcNs3EgZcPyfwFZJp+VvnQ88WWpVZhVLPWr0OeCO/IjRFmBBeSWZVS8pCBGxAfCNf23c8pllMxwEM8BBMAMcBDNgAvdZLnow76L7FwM0Go1C2+vp6Sm0PYDe3t7C26yD1whmOAhmgINgBjgIZoCDYAY4CGZA2tBRp0na0PTYJan443BmNUoZMedpoAtAUgewnWxAQbNxY6ybRucDz0TEc2UUY1aXsQbhcuDOMgoxq1NyEPJOOZcCy0eY7s771rbGska4GFgfES8MN9Gd962djSUIV+DNIhunUm/5eDhwIfCLcssxq0dqn+WXgaNLrsWsNj6zbIaDYAY4CGaAg2AGOAhmACgiim9UGgBSrkeaBuwovIBitXqNrV4f1FvjSREx6hneUoKQStLaiGjpW0m2eo2tXh+0R43eNDLDQTAD6g/CD2tefopWr7HV64M2qLHWfQSzVlH3GsGsJdQSBElzJT0tabOkNwxMWDdJJ0haJelJSRslXVd3TSOR1JGPdvrbumsZTrsMVl/5plF+A4C/kV3WvQ14FLgiIlpmXDZJxwHHRcR6SZOBdcBlrVTjIEnXk41mdGREzKu7nqEkLQX+GBFLBgerj4idddc1VB1rhLOAzRGxJR+8/C5gfg11jCgino+I9fnz3cAmYEa9Vb2RpE7gEmBJ3bUMp2mw+tsgG6y+FUMA9QRhBrC16fU2WvBLNkhSA5gFPLLvOWuxCLgBeL3uQkaQNFh9K/DO8j5IOgJYAfRExK6662kmaR7QHxHr6q5lH5IGq28FdQRhO3BC0+vO/L2WIukgshDcERGt2EX1HOBSSX1km5fnSVpWb0lv0DaD1dcRhEeBUyWdnO88XQ78uoY6RiRJZNu1myLilrrrGU5ELIyIzohokP0OH4qIj9dc1l7aabD6yoeOiog9kq4B7gc6gNsjYmPVdYziHOAq4HFJg2NCfTEi7q2xpnbVFoPV+8yyGd5ZNgMcBDPAQTADHAQzwEEwAxwEM8BBMAMcBDMA/g+m+WVbKo1BpQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 216x216 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sample_index = 45\n",
    "plt.figure(figsize=(3, 3))\n",
    "plt.imshow(digits.images[sample_index], cmap=plt.cm.gray_r,\n",
    "           interpolation='nearest')\n",
    "plt.title(\"image label: %d\" % digits.target[sample_index]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing\n",
    "\n",
    "- Normalization\n",
    "- Train / test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data = np.asarray(digits.data, dtype='float32')\n",
    "target = np.asarray(digits.target, dtype='int32')\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    data, target, test_size=0.15, random_state=37)\n",
    "\n",
    "# mean = 0 ; standard deviation = 1.0\n",
    "scaler = preprocessing.StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# print(scaler.mean_)\n",
    "# print(scaler.scale_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot(n_classes, y):\n",
    "    return np.eye(n_classes)[y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot(n_classes=10, y=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot(n_classes=10, y=[0, 4, 9, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "softmax of a single vector:\n",
      "[9.99662391e-01 3.35349373e-04 2.25956630e-06]\n"
     ]
    }
   ],
   "source": [
    "# norm over a column \n",
    "def softmax(X):\n",
    "    exp = np.exp(X)\n",
    "    return exp / np.sum(exp, axis=-1, keepdims=True)\n",
    "\n",
    "\n",
    "print(\"softmax of a single vector:\")\n",
    "print(softmax([10, 2, -3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0. 1. 2. 3. 4. 5. 6. 7. 8.]\n",
      "  [0. 1. 2. 3. 4. 5. 6. 7. 8.]\n",
      "  [0. 1. 2. 3. 4. 5. 6. 7. 8.]\n",
      "  [0. 1. 2. 3. 4. 5. 6. 7. 8.]\n",
      "  [0. 1. 2. 3. 4. 5. 6. 7. 8.]\n",
      "  [0. 1. 2. 3. 4. 5. 6. 7. 8.]]\n",
      "\n",
      " [[0. 1. 2. 3. 4. 5. 6. 7. 8.]\n",
      "  [0. 1. 2. 3. 4. 5. 6. 7. 8.]\n",
      "  [0. 1. 2. 3. 4. 5. 6. 7. 8.]\n",
      "  [0. 1. 2. 3. 4. 5. 6. 7. 8.]\n",
      "  [0. 1. 2. 3. 4. 5. 6. 7. 8.]\n",
      "  [0. 1. 2. 3. 4. 5. 6. 7. 8.]]\n",
      "\n",
      " [[0. 1. 2. 3. 4. 5. 6. 7. 8.]\n",
      "  [0. 1. 2. 3. 4. 5. 6. 7. 8.]\n",
      "  [0. 1. 2. 3. 4. 5. 6. 7. 8.]\n",
      "  [0. 1. 2. 3. 4. 5. 6. 7. 8.]\n",
      "  [0. 1. 2. 3. 4. 5. 6. 7. 8.]\n",
      "  [0. 1. 2. 3. 4. 5. 6. 7. 8.]]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[[36.],\n",
       "        [36.],\n",
       "        [36.],\n",
       "        [36.],\n",
       "        [36.],\n",
       "        [36.]],\n",
       "\n",
       "       [[36.],\n",
       "        [36.],\n",
       "        [36.],\n",
       "        [36.],\n",
       "        [36.],\n",
       "        [36.]],\n",
       "\n",
       "       [[36.],\n",
       "        [36.],\n",
       "        [36.],\n",
       "        [36.],\n",
       "        [36.],\n",
       "        [36.]]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TO BE EXAMINED MORE: keepdims?!\n",
    "\n",
    "m = 3\n",
    "W = np.ones((m, 2 * m, 3 * m))\n",
    "\n",
    "for i in range(m):\n",
    "    for j in range(2 * m):\n",
    "        for k in range(3 * m):\n",
    "            W[i, j, k] = k\n",
    "        \n",
    "# np.sum(W, axis=0)\n",
    "print(W)\n",
    "\n",
    "3 * 6 * 9\n",
    "np.sum(W, axis=-1, keepdims=True)\n",
    "\n",
    "# print(W[:, 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Probabilities should sum to 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "print(np.sum(softmax([10, 2, -3])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sotfmax of 2 vectors:\n",
      "[[9.99662391e-01 3.35349373e-04 2.25956630e-06]\n",
      " [2.47262316e-03 9.97527377e-01 1.38536042e-11]]\n"
     ]
    }
   ],
   "source": [
    "print(\"sotfmax of 2 vectors:\")\n",
    "X = np.array([[10, 2, -3],\n",
    "              [-1, 5, -20]])\n",
    "print(softmax(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sum of probabilities for each input vector of logits should some to 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 1.]\n"
     ]
    }
   ],
   "source": [
    "print(np.sum(softmax(X), axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement a function that given the true one-hot encoded class `Y_true` and and some predicted probabilities `Y_pred` returns the negative log likelihood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def nll(Y_true, Y_pred):\n",
    "#     Y_true = np.asarray(Y_true)\n",
    "#     Y_pred = np.asarray(Y_pred)\n",
    "\n",
    "#     return -np.sum(Y_true * np.log(Y_pred))\n",
    "\n",
    "\n",
    "# # Make sure that it works for a simple sample at a time\n",
    "# print(nll([1, 0, 0], [.99, 0.01, 0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check that the nll of a very confident yet bad prediction is a much higher positive number:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(nll([1, 0, 0], [0.01, 0.01, .98]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure that your implementation can compute the average negative log likelihood of a group of predictions: `Y_pred` and `Y_true` can therefore be past as 2D arrays:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def nll(Y_true, Y_pred):\n",
    "#     Y_true = np.atleast_2d(Y_true)\n",
    "#     Y_pred = np.atleast_2d(Y_pred)\n",
    "\n",
    "#     # TODO\n",
    "#     return 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Check that the average NLL of the following 3 almost perfect\n",
    "# # predictions is close to 0\n",
    "# Y_true = np.array([[0, 1, 0],\n",
    "#                    [1, 0, 0],\n",
    "#                    [0, 0, 1]])\n",
    "\n",
    "# Y_pred = np.array([[0,   1,    0],\n",
    "#                    [.99, 0.01, 0],\n",
    "#                    [0,   0,    1]])\n",
    "\n",
    "# print(nll(Y_true, Y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # %load solutions/numpy_nll.py\n",
    "# EPSILON = 1e-8\n",
    "\n",
    "\n",
    "# def nll(Y_true, Y_pred):\n",
    "#     Y_true, Y_pred = np.atleast_2d(Y_true), np.atleast_2d(Y_pred)\n",
    "#     loglikelihoods = np.sum(np.log(EPSILON + Y_pred) * Y_true, axis=1)\n",
    "#     return -np.mean(loglikelihoods)\n",
    "\n",
    "\n",
    "# # Make sure that it works for a simple sample at a time\n",
    "# print(nll([1, 0, 0], [.99, 0.01, 0]))\n",
    "\n",
    "# # Check that the nll of a very confident yet bad prediction\n",
    "# # is very high:\n",
    "# print(nll([1, 0, 0], [0.01, 0.01, .98]))\n",
    "\n",
    "# # Check that the average NLL of the following 3 almost perfect\n",
    "# # predictions is close to 0\n",
    "# Y_true = np.array([[0, 1, 0],\n",
    "#                    [1, 0, 0],\n",
    "#                    [0, 0, 1]])\n",
    "\n",
    "# Y_pred = np.array([[0,   1,    0],\n",
    "#                    [.99, 0.01, 0],\n",
    "#                    [0,   0,    1]])\n",
    "# print(nll(Y_true, Y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now study the following linear model trainable by SGD, **one sample at a time**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class LogisticRegression():\n",
    "\n",
    "    \n",
    "#     def __init__(self, input_size, output_size):\n",
    "#         self.W = np.random.uniform(size=(input_size, output_size),\n",
    "#                                    high=0.1, low=-0.1)\n",
    "#         self.b = np.random.uniform(size=output_size,\n",
    "#                                    high=0.1, low=-0.1)\n",
    "        \n",
    "#         self.output_size = output_size\n",
    "        \n",
    "#     def forward(self, X):\n",
    "#         Z = np.dot(X, self.W) + self.b\n",
    "#         return softmax(Z)\n",
    "    \n",
    "#     # this returns a vector 1500x1:\n",
    "#     # that is the maximum prob. over axis=1 which is on [0, 1, 2, \\dots, 9]\n",
    "#     # and the axis=0 is say 1500 where we have 1500 images each of these\n",
    "#     # image was originally 8x8 and then transformed into 1x64\n",
    "#     def predict(self, X):\n",
    "#         if len(X.shape) == 1:\n",
    "#             return np.argmax(self.forward(X))\n",
    "#         else:\n",
    "#             return np.argmax(self.forward(X), axis=1)\n",
    "    \n",
    "#     def grad_loss(self, x, y_true):\n",
    "#         y_pred = self.forward(x)\n",
    "#         dnll_output =  y_pred - one_hot(self.output_size, y_true)\n",
    "#         grad_W = np.outer(x, dnll_output)\n",
    "#         grad_b = dnll_output\n",
    "#         grads = {\"W\": grad_W, \"b\": grad_b}\n",
    "        \n",
    "#         return grads\n",
    "    \n",
    "#     def train(self, x, y, learning_rate):\n",
    "#         # Traditional SGD update without momentum\n",
    "#         grads = self.grad_loss(x, y)\n",
    "#         self.W = self.W - learning_rate * grads[\"W\"]\n",
    "#         self.b = self.b - learning_rate * grads[\"b\"]      \n",
    "        \n",
    "#     def loss(self, X, y):\n",
    "#         return nll(one_hot(self.output_size, y), self.forward(X))\n",
    "\n",
    "#     def accuracy(self, X, y):\n",
    "#         y_preds = np.argmax(self.forward(X), axis=1)\n",
    "#         return np.mean(y_preds == y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "formula (15) is from https://www.kamperh.com/notes/kamper_backprop17.pdf"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](attachment:image.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Build a model and test its forward inference\n",
    "# n_features = X_train.shape[1]\n",
    "# n_classes  = len(np.unique(y_train))\n",
    "# lr         = LogisticRegression(n_features, n_classes)\n",
    "\n",
    "# print(\"Evaluation of the untrained model:\")\n",
    "# train_loss = lr.loss(X_train, y_train)\n",
    "# train_acc  = lr.accuracy(X_train, y_train)\n",
    "# test_acc   = lr.accuracy(X_test, y_test)\n",
    "\n",
    "# print(\"train loss: %0.4f, train acc: %0.3f, test acc: %0.3f\"\n",
    "#       % (train_loss, train_acc, test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the randomly initialized model on the first example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def plot_prediction(model, sample_idx=0, classes=range(10)):\n",
    "#     fig, (ax0, ax1) = plt.subplots(nrows=1, ncols=2, figsize=(10, 4))\n",
    "\n",
    "#     ax0.imshow(scaler.inverse_transform(X_test[sample_idx]).reshape(8, 8), cmap=plt.cm.gray_r,\n",
    "#                interpolation='nearest')\n",
    "#     ax0.set_title(\"True image label: %d\" % y_test[sample_idx]);\n",
    "\n",
    "\n",
    "#     ax1.bar(classes, one_hot(len(classes), y_test[sample_idx]), label='true')\n",
    "#     ax1.bar(classes, model.forward(X_test[sample_idx]), label='prediction', color=\"red\")\n",
    "#     ax1.set_xticks(classes)\n",
    "#     prediction = model.predict(X_test[sample_idx])\n",
    "#     ax1.set_title('Output probabilities (prediction: %d)'\n",
    "#                   % prediction)\n",
    "#     ax1.set_xlabel('Digit class')\n",
    "#     ax1.legend()\n",
    "    \n",
    "# plot_prediction(lr, sample_idx=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Training for one epoch\n",
    "# learning_rate = 0.01\n",
    "\n",
    "# for i, (x, y) in enumerate(zip(X_train, y_train)):\n",
    "#     lr.train(x, y, learning_rate)\n",
    "#     if i % 100 == 0:\n",
    "#         train_loss = lr.loss(X_train, y_train)\n",
    "#         train_acc = lr.accuracy(X_train, y_train)\n",
    "#         test_acc = lr.accuracy(X_test, y_test)\n",
    "#         print(\"Update #%d, train loss: %0.4f, train acc: %0.3f, test acc: %0.3f\"\n",
    "#               % (i, train_loss, train_acc, test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the trained model on the first example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_prediction(lr, sample_idx=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## b) Feedforward Multilayer\n",
    "\n",
    "The objective of this section is to implement the backpropagation algorithm (SGD with the chain rule) on a single layer neural network using the sigmoid activation function.\n",
    "\n",
    "- Implement the `sigmoid` and its element-wise derivative `dsigmoid` functions:\n",
    "\n",
    "$$\n",
    "sigmoid(x) = \\frac{1}{1 + e^{-x}}\n",
    "$$\n",
    "\n",
    "$$\n",
    "dsigmoid(x) = sigmoid(x) \\cdot (1 - sigmoid(x))\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def sigmoid(X):\n",
    "    return 1 / (1 + np.exp(-X))\n",
    "    \n",
    "\n",
    "def dsigmoid(X):\n",
    "    return sigmoid(X) * (1 - sigmoid(X))\n",
    "\n",
    "\n",
    "x = np.linspace(-5, 5, 100)\n",
    "plt.plot(x, sigmoid(x), label='sigmoid')\n",
    "plt.plot(x, dsigmoid(x), label='dsigmoid')\n",
    "plt.legend(loc='best');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# %load solutions/sigmoid.py\n",
    "def sigmoid(X):\n",
    "    return 1 / (1 + np.exp(-X))\n",
    "\n",
    "\n",
    "def dsigmoid(X):\n",
    "    sig=sigmoid(X)\n",
    "    return sig * (1 - sig)\n",
    "\n",
    "\n",
    "x = np.linspace(-5, 5, 100)\n",
    "plt.plot(x, sigmoid(x), label='sigmoid')\n",
    "plt.plot(x, dsigmoid(x), label='dsigmoid')\n",
    "plt.legend(loc='best');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Implement `forward` and `forward_keep_all` functions for a model with a hidden layer with a sigmoid activation function:\n",
    "  - $\\mathbf{h} = sigmoid(\\mathbf{W}^h \\mathbf{x} + \\mathbf{b^h})$\n",
    "  - $\\mathbf{y} = softmax(\\mathbf{W}^o \\mathbf{h} + \\mathbf{b^o})$\n",
    "\n",
    "- Notes: \n",
    "  - try to keep the code as similar as possible as the previous one;\n",
    "  - `forward` now has a keep activations parameter to also return hidden activations and pre activations;\n",
    "\n",
    "- Update the grad function to compute all gradients; check that the gradients are well defined;\n",
    "\n",
    "- Implement the `train` and `loss` functions.\n",
    "\n",
    "**Bonus**: reimplementing all from scratch only using the lecture slides but without looking at the solution of the `LogisticRegression` is an excellent exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPSILON = 1e-8\n",
    "\n",
    "\n",
    "class NeuralNet():\n",
    "    \"\"\"MLP with 1 hidden layer with a sigmoid activation\"\"\"\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        # TODO\n",
    "        self.W_h = None\n",
    "        self.b_h = None\n",
    "        self.W_o = None\n",
    "        self.b_o = None\n",
    "        self.output_size = output_size\n",
    "            \n",
    "    def forward_keep_activations(self, X):\n",
    "        # TODO\n",
    "        z_h = W_o * h + b\n",
    "        h = softmax(z_h)\n",
    "        y = np.zeros(size=self.output_size)\n",
    "        return y, h, z_h\n",
    "\n",
    "    def forward(self, X):\n",
    "        y, h, z_h = self.forward_keep_activations(X)\n",
    "        return y\n",
    "    \n",
    "    def loss(self, X, y):\n",
    "        # TODO\n",
    "        return 42.\n",
    "\n",
    "    def grad_loss(self, x, y_true):\n",
    "        # TODO\n",
    "        return {\"W_h\": 0., \"b_h\": 0., \"W_o\": 0., \"b_o\": 0.}\n",
    "\n",
    "    def train(self, x, y, learning_rate):\n",
    "        # TODO\n",
    "        pass\n",
    "\n",
    "    def predict(self, X):\n",
    "        if len(X.shape) == 1:\n",
    "            return np.argmax(self.forward(X))\n",
    "        else:\n",
    "            return np.argmax(self.forward(X), axis=1)\n",
    "\n",
    "    def accuracy(self, X, y):\n",
    "        y_preds = np.argmax(self.forward(X), axis=1)\n",
    "        return np.mean(y_preds == y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/neural_net.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # %load solutions/neural_net.py\n",
    "class NeuralNet():\n",
    "    \"\"\"MLP with 1 hidden layer with a sigmoid activation\"\"\"\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        self.W_h = np.random.uniform(\n",
    "            size=(input_size, hidden_size), high=0.01, low=-0.01)\n",
    "        self.b_h = np.zeros(hidden_size)\n",
    "        self.W_o = np.random.uniform(\n",
    "            size=(hidden_size, output_size), high=0.01, low=-0.01)\n",
    "        self.b_o = np.zeros(output_size)\n",
    "        self.output_size = output_size\n",
    "\n",
    "    def forward(self, X, keep_activations=False):\n",
    "        z_h = np.dot(X, self.W_h) + self.b_h\n",
    "        h = sigmoid(z_h)\n",
    "        z_o = np.dot(h, self.W_o) + self.b_o\n",
    "        y = softmax(z_o)\n",
    "        if keep_activations:\n",
    "            return y, h, z_h\n",
    "        else:\n",
    "            return y\n",
    "\n",
    "    def loss(self, X, y):\n",
    "        return nll(one_hot(self.output_size, y), self.forward(X))\n",
    "\n",
    "    \n",
    "    def grad_loss(self, x, y_true):\n",
    "        y, h, z_h = self.forward(x, keep_activations=True)\n",
    "        grad_z_o = y - one_hot(self.output_size, y_true)\n",
    "\n",
    "        grad_W_o = np.outer(h, grad_z_o)\n",
    "        grad_b_o = grad_z_o\n",
    "        \n",
    "        grad_h = np.dot(grad_z_o, np.transpose(self.W_o))\n",
    "        grad_z_h = grad_h * dsigmoid(z_h)\n",
    "        \n",
    "        grad_W_h = np.outer(x, grad_z_h)\n",
    "        grad_b_h = grad_z_h\n",
    "        \n",
    "        grads = {\"W_h\": grad_W_h, \"b_h\": grad_b_h,\n",
    "                 \"W_o\": grad_W_o, \"b_o\": grad_b_o}\n",
    "        return grads\n",
    "\n",
    "    def train(self, x, y, learning_rate):\n",
    "        # Traditional SGD update on one sample at a time\n",
    "        grads = self.grad_loss(x, y)\n",
    "        self.W_h = self.W_h - learning_rate * grads[\"W_h\"]\n",
    "        self.b_h = self.b_h - learning_rate * grads[\"b_h\"]\n",
    "        self.W_o = self.W_o - learning_rate * grads[\"W_o\"]\n",
    "        self.b_o = self.b_o - learning_rate * grads[\"b_o\"]\n",
    "\n",
    "    def predict(self, X):\n",
    "        if len(X.shape) == 1:\n",
    "            return np.argmax(self.forward(X))\n",
    "        else:\n",
    "            return np.argmax(self.forward(X), axis=1)\n",
    "\n",
    "    def accuracy(self, X, y):\n",
    "        y_preds = np.argmax(self.forward(X), axis=1)\n",
    "        return np.mean(y_preds == y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fn_softmax(np.ones(10)) * fn_softmax(np.ones(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n",
      "8\n",
      "7\n",
      "6\n",
      "5\n",
      "4\n",
      "3\n",
      "2\n",
      "1\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "for i in reversed(range(10)):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: DEFINE DIFFERENT FUNCTIONS AND THEIR DERIVS.        \n",
    "act_fn = lambda u: 1 / (1 + np.exp(-u))\n",
    "derivative_act_fn = lambda u: act_fn(u) * (1 - act_fn(u))\n",
    "fn_softmax = lambda u: np.exp(u) / sum(np.exp(u))\n",
    "derivative_fn_softmax = lambda u: fn_softmax(u) * (1 - fn_softmax(u))\n",
    "\n",
    "\n",
    "class NeuralNetMultiLayer():\n",
    "    \"\"\"MLP\"\"\"\n",
    "    \n",
    "    def __init__(self, layers_specs, dim_input=None):\n",
    "        \"\"\"\n",
    "        network: is a list of layers\n",
    "        \"\"\"\n",
    "        dim_prev_layer = dim_input\n",
    "        self.network = []\n",
    "        for index, layer_spec in enumerate(layers_specs):\n",
    "            number_of_neurons, act_fn = layer_spec\n",
    "            self.network.append(self.layer(layer_spec, dim_prev_layer, index=index))\n",
    "            dim_prev_layer = number_of_neurons\n",
    "       \n",
    "        self.output_size, _ = layers_specs[-1]\n",
    "        \n",
    "    def layer(self, spec, dim_input=None, index=0, init_distribution=(-0.01, 0.01)):\n",
    "        \"\"\"\n",
    "        sig, th, relu, \n",
    "        \"\"\"\n",
    "        \n",
    "        number_of_neurons, act_fn = spec\n",
    "        low, high = init_distribution\n",
    "        \n",
    "        print(\"layer index:\", index)\n",
    "        print(\"spec :\", spec)\n",
    "        print(\"W dim:\", number_of_neurons, dim_input)\n",
    "        \n",
    "        W = np.random.uniform(low=low, high=high, size=(number_of_neurons, dim_input)) # W*x\n",
    "\n",
    "        # this is very important; \n",
    "        # size specifies only one dimension \n",
    "        # so numpy creates a vector; \n",
    "        # e.g. (3, 1) is still treated as a matrix and affects MUL\n",
    "        # but (3, ) is a vector!\n",
    "        # so the following is wrong\n",
    "        # bias = np.random.uniform(low=low, high=high, size=(number_of_neurons, 1))\n",
    "        # but this is correct:\n",
    "        bias = np.random.uniform(low=low, high=high, size=(number_of_neurons, ))\n",
    "        return (W, bias, act_fn)\n",
    "    \n",
    "    def forward(self, x_input, init_distribution=(-0.01, 0.01)):\n",
    "        \n",
    "        \"\"\"additionally we store intermediate \n",
    "        computations over each layer in the list\n",
    "        `output_over_layer`;\n",
    "        we need the results to calculate \n",
    "        gradient of individual layers\n",
    "        in a back-propagation (below)\n",
    "        \"\"\"\n",
    "        \n",
    "        low, high = init_distribution\n",
    "        \n",
    "        lst_x = []\n",
    "        lst_z = []\n",
    "        lst_y = []\n",
    "        \n",
    "        \n",
    "        x = x_input \n",
    "        for index, layer in enumerate(self.network):\n",
    "            lst_x.append(x)\n",
    "            (W, bias, act_fn) = layer\n",
    "#             print(\"index     :\", index)\n",
    "#             print(\"W.shape   :\", W.shape)\n",
    "#             print(\"tmp.shape :\", tmp.shape)\n",
    "#             print(\"prod.shape:\", np.dot(W, tmp).shape)\n",
    "#             print(\"bias.shape:\", bias.shape)\n",
    "#             print(\"act. fnct :\", act_fn)\n",
    "            \n",
    "            z = np.dot(W, x) + bias\n",
    "            lst_z.append(z)\n",
    "            y = act_fn(z)\n",
    "            lst_y.append(y)\n",
    "            x = y\n",
    "        \n",
    "        return lst_x, lst_z, lst_y\n",
    "    \n",
    "    \n",
    "    # TODO: check the rate init. value!\n",
    "    def backpropagate(self, x, y_target, eta=0.01): \n",
    "     \n",
    "        lst_x, lst_z, lst_y = self.forward(x) \n",
    "        # get the last elements\n",
    "        x = lst_x[-1]\n",
    "        z = lst_z[-1]\n",
    "        y = lst_y[-1]\n",
    "        \n",
    "        \n",
    "        #-----------------------------------------------------\n",
    "        # list of deltas\n",
    "        #-----------------------------------------------------\n",
    "        lst_delta = []\n",
    "        #-----------------------------------------------------\n",
    "        \n",
    "        \n",
    "        #-----------------------------------------------------\n",
    "        # J stands for value of the optimization function\n",
    "        # Energy, Loss, Error;\n",
    "        J = np.sum((y - y_target) ** 2)\n",
    "        derivative_J = 2 * (y - y_target) # this is delta(loss) / delta(a_last) \n",
    "        \n",
    "        # importantly: here * is the component-wise vector multiplication \n",
    "        # it is not dot-product\n",
    "        \n",
    "        delta = derivative_J * derivative_fn_softmax(z) \n",
    "        lst_delta.append(delta)\n",
    "        \n",
    "        nabla_JW = np.outer(delta, x)\n",
    "        nabla_Jb = delta\n",
    "        \n",
    "            \n",
    "        #-----------------------------------------------------\n",
    "        # to update the last layer\n",
    "        #-----------------------------------------------------\n",
    "        layer = self.network[-1]\n",
    "        (W, bias, act_fn) = layer\n",
    "        W = W - eta * nabla_JW\n",
    "        bias = bias - eta * nabla_Jb\n",
    "        self.network[-1] = (W, bias, act_fn)\n",
    "        #-----------------------------------------------------\n",
    "        \n",
    "    \n",
    "#         print(\"* shapes W:\", W.shape)\n",
    "#         print(\"* shapes delta_z:\", delta_z.shape)\n",
    "#         print(\"* shapes y_prev:\", y_prev.shape)\n",
    "#         print(\"* shapes W:\", W.shape)\n",
    "#         print(\"* shapes delta_z:\", delta_z.shape)\n",
    "#         print(\"* shapes y_prev:\", y_prev.shape)\n",
    "            \n",
    "    \n",
    "        # in the following loop we do a backprop and *prepend* deltas \n",
    "        # index is going in the reverse way: \n",
    "        for ind in reversed(range(len(self.network) - 1)): \n",
    "#             print(\"index:\", ind)\n",
    "            x = lst_x[ind]\n",
    "            z = lst_z[ind]\n",
    "            y = lst_y[ind]\n",
    "        \n",
    "#             if ind > 0: \n",
    "#                 y_prev = lst_y[ind - 1]\n",
    "#             else: \n",
    "#                 y_prev = x\n",
    "                \n",
    "            #-----------------------------------------------------\n",
    "            layer = self.network[ind]\n",
    "            (W, bias, act_fn) = layer\n",
    "            \n",
    "            layer_next = self.network[ind + 1] \n",
    "            (W_next, bias_next, act_fn_next) = layer_next\n",
    "            #-----------------------------------------------------\n",
    "      \n",
    "            # vector_one := derivative_sigma^{nu}(z^{nu}) \n",
    "            # vector_two := (transpose(W^{nu + 1}) * delta^{nu + 1})\n",
    "            # delta^{nu} := component_multiplication(vector_one, vector_two)\n",
    "\n",
    "            # b/c we prepend delta to the list lst_delta and loop-backward\n",
    "            delta_next = lst_delta[0]\n",
    "            \n",
    "            # here * is not a dot-product\n",
    "            delta = derivative_act_fn(z) * np.dot(np.transpose(W_next), delta_next) \n",
    "            #prepend delta to the list\n",
    "            lst_delta = [delta] + lst_delta\n",
    "\n",
    "            nabla_JW = np.outer(delta, x)\n",
    "            nabla_Jb = delta\n",
    "            \n",
    "            W = W - eta * nabla_JW\n",
    "            bias = bias - eta * nabla_Jb\n",
    "            self.network[ind] = (W, bias, act_fn)\n",
    "            \n",
    "            \n",
    "#             print(\"** shapes W:\", W.shape)\n",
    "#             print(\"** shapes delta_next_z:\", delta_z_next.shape)\n",
    "#             print(\"** shapes y_prev:\", y_prev.shape)\n",
    "#             print(\"** shapes delta_z:\", delta_z.shape\n",
    "#             print(\"B: shapes:\", delta_z.shape, y_prev.T.shape)\n",
    "\n",
    "            # derivate: map from function to function\n",
    "            # derivative_act_fn = map_derivative[act_fn]\n",
    "            \n",
    "        \n",
    "        return 0\n",
    "            \n",
    "            \n",
    "# ----------------------------------------------------------------------------------\n",
    "    # TODO: WE WILL SPECIFY A LOSS_FUNCTION \n",
    "    # WE USE A SQUARE ERROR FUNCTION \n",
    "    # LATER WE WILL SPECIFY AND WRITE THIS loss-fn\n",
    "# ----------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------\n",
    "# out = sigma_3(a_3)\n",
    "# out = sigma_3 (W_3 * x_3)\n",
    "# out = softmax (z_3) = a_3 = y_3\n",
    "# --------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer index: 0\n",
      "spec : (20, <function <lambda> at 0x7fa06d679320>)\n",
      "W dim: 20 64\n",
      "layer index: 1\n",
      "spec : (25, <function <lambda> at 0x7fa06d679320>)\n",
      "W dim: 25 20\n",
      "layer index: 2\n",
      "spec : (10, <function <lambda> at 0x7fa06d6dd290>)\n",
      "W dim: 10 25\n"
     ]
    }
   ],
   "source": [
    "init_distribution = (-0.01, 0.01)\n",
    "layers_specs = [(20, act_fn), \n",
    "                (25, act_fn), \n",
    "#                 (30, act_fn), \n",
    "                (10, fn_softmax),\n",
    "                ]\n",
    "\n",
    "nn = NeuralNetMultiLayer(layers_specs=layers_specs, dim_input=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # test input:\n",
    "# print(X_train.size)\n",
    "# x_input = X_train[0]\n",
    "# print(type(x_input), x_input.shape)\n",
    "# print(x_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# z_lst, y_lst = nn.forward(x_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# z_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(sum(y))\n",
    "# print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### this is the backprop part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_input = X_train[0]\n",
    "# x_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_target = y_train[0]\n",
    "y_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "lst_delta = nn.backpropagate(x_input, y_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lst_delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer index: 0\n",
      "spec : (20, <function <lambda> at 0x7fa06d679320>)\n",
      "W dim: 20 64\n",
      "layer index: 1\n",
      "spec : (25, <function <lambda> at 0x7fa06d679320>)\n",
      "W dim: 25 20\n",
      "layer index: 2\n",
      "spec : (30, <function <lambda> at 0x7fa06d679320>)\n",
      "W dim: 30 25\n",
      "layer index: 3\n",
      "spec : (10, <function <lambda> at 0x7fa06d6dd290>)\n",
      "W dim: 10 30\n",
      "################################################################################\n",
      "i-th element: 0\n",
      "y: [7] [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "--------------------------------------------------------------------------------\n",
      "results after 100 iterations; argmax and prob. distr for softmax:\n",
      "--------------------------------------------------------------------------------\n",
      "7 [0.0532848  0.05142347 0.05287687 0.05199344 0.0545426  0.052849\n",
      " 0.05283433 0.52163864 0.05455444 0.05400242]\n",
      "################################################################################\n",
      "i-th element: 1\n",
      "y: [0] [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "--------------------------------------------------------------------------------\n",
      "results after 100 iterations; argmax and prob. distr for softmax:\n",
      "--------------------------------------------------------------------------------\n",
      "0 [0.35729102 0.05626632 0.05765869 0.05681332 0.05924101 0.05763326\n",
      " 0.05762031 0.17949819 0.05924812 0.05872975]\n",
      "################################################################################\n",
      "i-th element: 2\n",
      "y: [0] [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "--------------------------------------------------------------------------------\n",
      "results after 100 iterations; argmax and prob. distr for softmax:\n",
      "--------------------------------------------------------------------------------\n",
      "0 [0.75792008 0.02201256 0.02253717 0.02222238 0.02313947 0.02253614\n",
      " 0.02252729 0.06100552 0.02314711 0.02295229]\n",
      "################################################################################\n",
      "i-th element: 3\n",
      "y: [1] [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "--------------------------------------------------------------------------------\n",
      "results after 100 iterations; argmax and prob. distr for softmax:\n",
      "--------------------------------------------------------------------------------\n",
      "0 [0.27586204 0.17521322 0.05906017 0.05828765 0.06048824 0.05903343\n",
      " 0.0590227  0.13251456 0.0604922  0.06002578]\n",
      "################################################################################\n",
      "i-th element: 4\n",
      "y: [9] [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      "--------------------------------------------------------------------------------\n",
      "results after 100 iterations; argmax and prob. distr for softmax:\n",
      "--------------------------------------------------------------------------------\n",
      "9 [0.14523714 0.11911653 0.05367901 0.05305163 0.05483189 0.05365763\n",
      " 0.05364741 0.1010902  0.05483341 0.31085514]\n",
      "################################################################################\n",
      "i-th element: 5\n",
      "y: [8] [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      "--------------------------------------------------------------------------------\n",
      "results after 100 iterations; argmax and prob. distr for softmax:\n",
      "--------------------------------------------------------------------------------\n",
      "8 [0.11373756 0.10028727 0.05288688 0.05232836 0.05390441 0.05286587\n",
      " 0.05285774 0.08932338 0.27282124 0.15898729]\n",
      "################################################################################\n",
      "i-th element: 6\n",
      "y: [5] [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      "--------------------------------------------------------------------------------\n",
      "results after 100 iterations; argmax and prob. distr for softmax:\n",
      "--------------------------------------------------------------------------------\n",
      "5 [0.09828175 0.08976559 0.05265875 0.05215442 0.05357116 0.24260813\n",
      " 0.05263091 0.08218373 0.15477681 0.12136877]\n",
      "################################################################################\n",
      "i-th element: 7\n",
      "y: [3] [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      "--------------------------------------------------------------------------------\n",
      "results after 100 iterations; argmax and prob. distr for softmax:\n",
      "--------------------------------------------------------------------------------\n",
      "3 [0.08806831 0.08213656 0.05212599 0.22586884 0.05294387 0.14839194\n",
      " 0.05210178 0.07655838 0.11935757 0.10244676]\n",
      "################################################################################\n",
      "i-th element: 8\n",
      "y: [4] [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      "--------------------------------------------------------------------------------\n",
      "results after 100 iterations; argmax and prob. distr for softmax:\n",
      "--------------------------------------------------------------------------------\n",
      "4 [0.0796286  0.07528654 0.05074206 0.14253073 0.2258649  0.11489086\n",
      " 0.05072004 0.07104678 0.09982435 0.08946514]\n",
      "################################################################################\n",
      "i-th element: 9\n",
      "y: [0] [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "--------------------------------------------------------------------------------\n",
      "results after 100 iterations; argmax and prob. distr for softmax:\n",
      "--------------------------------------------------------------------------------\n",
      "0 [0.43479677 0.05165266 0.03614119 0.08462632 0.10975933 0.07273001\n",
      " 0.03613164 0.04910223 0.06529123 0.05976863]\n",
      "################################################################################\n",
      "i-th element: 10\n",
      "y: [8] [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      "--------------------------------------------------------------------------------\n",
      "results after 100 iterations; argmax and prob. distr for softmax:\n",
      "--------------------------------------------------------------------------------\n",
      "8 [0.15403614 0.04606639 0.03305115 0.07015703 0.0851244  0.06205584\n",
      " 0.03304188 0.04399628 0.42000326 0.05246763]\n",
      "################################################################################\n",
      "i-th element: 11\n",
      "y: [6] [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "--------------------------------------------------------------------------------\n",
      "results after 100 iterations; argmax and prob. distr for softmax:\n",
      "--------------------------------------------------------------------------------\n",
      "8 [0.15006091 0.06265462 0.04624095 0.08951254 0.10382737 0.08100818\n",
      " 0.12870902 0.06012877 0.20760806 0.07024958]\n",
      "################################################################################\n",
      "i-th element: 12\n",
      "y: [4] [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      "--------------------------------------------------------------------------------\n",
      "results after 100 iterations; argmax and prob. distr for softmax:\n",
      "--------------------------------------------------------------------------------\n",
      "4 [0.07196287 0.03523126 0.02645203 0.04837304 0.54827488 0.04438901\n",
      " 0.06456929 0.03391184 0.08772201 0.03911376]\n",
      "################################################################################\n",
      "i-th element: 13\n",
      "y: [1] [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "--------------------------------------------------------------------------------\n",
      "results after 100 iterations; argmax and prob. distr for softmax:\n",
      "--------------------------------------------------------------------------------\n",
      "4 [0.0979295  0.19302346 0.04100423 0.07095731 0.21770398 0.06584911\n",
      " 0.09007742 0.05168954 0.11292045 0.058845  ]\n",
      "################################################################################\n",
      "i-th element: 14\n",
      "y: [1] [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "--------------------------------------------------------------------------------\n",
      "results after 100 iterations; argmax and prob. distr for softmax:\n",
      "--------------------------------------------------------------------------------\n",
      "1 [0.03879034 0.69198776 0.01702523 0.02894682 0.07078895 0.02698094\n",
      " 0.03602361 0.02137593 0.04384549 0.02423493]\n",
      "################################################################################\n",
      "i-th element: 15\n",
      "y: [6] [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "--------------------------------------------------------------------------------\n",
      "results after 100 iterations; argmax and prob. distr for softmax:\n",
      "--------------------------------------------------------------------------------\n",
      "6 [0.06631136 0.22162181 0.03102936 0.05108809 0.1064619  0.04789264\n",
      " 0.32010589 0.03853714 0.07359769 0.04335412]\n",
      "################################################################################\n",
      "i-th element: 16\n",
      "y: [7] [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "--------------------------------------------------------------------------------\n",
      "results after 100 iterations; argmax and prob. distr for softmax:\n",
      "--------------------------------------------------------------------------------\n",
      "6 [0.07857061 0.16570346 0.03966093 0.06278675 0.11254321 0.05927081\n",
      " 0.18631985 0.15543336 0.08554833 0.05416268]\n",
      "################################################################################\n",
      "i-th element: 17\n",
      "y: [2] [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      "--------------------------------------------------------------------------------\n",
      "results after 100 iterations; argmax and prob. distr for softmax:\n",
      "--------------------------------------------------------------------------------\n",
      "6 [0.08100876 0.13518455 0.12693507 0.0670558  0.10639647 0.06376003\n",
      " 0.14358443 0.13048931 0.08673333 0.05885226]\n",
      "################################################################################\n",
      "i-th element: 18\n",
      "y: [1] [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "--------------------------------------------------------------------------------\n",
      "results after 100 iterations; argmax and prob. distr for softmax:\n",
      "--------------------------------------------------------------------------------\n",
      "1 [0.03717153 0.61827133 0.05424389 0.03127627 0.04706282 0.02984009\n",
      " 0.05954173 0.05541928 0.03950431 0.02766875]\n",
      "################################################################################\n",
      "i-th element: 19\n",
      "y: [1] [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "--------------------------------------------------------------------------------\n",
      "results after 100 iterations; argmax and prob. distr for softmax:\n",
      "--------------------------------------------------------------------------------\n",
      "1 [0.01839618 0.81188554 0.02663979 0.01549584 0.02320491 0.01478623\n",
      " 0.02914135 0.02719995 0.01954001 0.0137102 ]\n",
      "################################################################################\n",
      "i-th element: 20\n",
      "y: [4] [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      "--------------------------------------------------------------------------------\n",
      "results after 100 iterations; argmax and prob. distr for softmax:\n",
      "--------------------------------------------------------------------------------\n",
      "1 [0.05339999 0.28446566 0.07449989 0.04551758 0.24522975 0.04355501\n",
      " 0.08048785 0.0758472  0.05643432 0.04056274]\n",
      "################################################################################\n",
      "i-th element: 21\n",
      "y: [0] [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "results after 100 iterations; argmax and prob. distr for softmax:\n",
      "--------------------------------------------------------------------------------\n",
      "0 [0.27811595 0.15718129 0.0725573  0.04711722 0.14934248 0.04523935\n",
      " 0.07725491 0.07363318 0.05721865 0.04233967]\n",
      "################################################################################\n",
      "i-th element: 22\n",
      "y: [4] [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      "--------------------------------------------------------------------------------\n",
      "results after 100 iterations; argmax and prob. distr for softmax:\n",
      "--------------------------------------------------------------------------------\n",
      "4 [0.08720214 0.06265124 0.03320948 0.02207219 0.6583239  0.02121696\n",
      " 0.03516712 0.03366393 0.02660402 0.01988902]\n",
      "################################################################################\n",
      "i-th element: 23\n",
      "y: [7] [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "--------------------------------------------------------------------------------\n",
      "results after 100 iterations; argmax and prob. distr for softmax:\n",
      "--------------------------------------------------------------------------------\n",
      "7 [0.12499712 0.09972024 0.05893854 0.04043895 0.22743433 0.03895581\n",
      " 0.06201919 0.2627099  0.04814829 0.03663764]\n",
      "################################################################################\n",
      "i-th element: 24\n",
      "y: [5] [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      "--------------------------------------------------------------------------------\n",
      "results after 100 iterations; argmax and prob. distr for softmax:\n",
      "--------------------------------------------------------------------------------\n",
      "7 [0.12140716 0.10430692 0.06859795 0.04888597 0.16407338 0.14637964\n",
      " 0.07166516 0.17274634 0.05735003 0.04458744]\n",
      "################################################################################\n",
      "i-th element: 25\n",
      "y: [3] [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      "--------------------------------------------------------------------------------\n",
      "results after 100 iterations; argmax and prob. distr for softmax:\n",
      "--------------------------------------------------------------------------------\n",
      "3 [0.1042024  0.09359009 0.06691334 0.1911221  0.12551133 0.11748458\n",
      " 0.06945447 0.12908106 0.05719487 0.04544576]\n",
      "################################################################################\n",
      "i-th element: 26\n",
      "y: [8] [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      "--------------------------------------------------------------------------------\n",
      "results after 100 iterations; argmax and prob. distr for softmax:\n",
      "--------------------------------------------------------------------------------\n",
      "8 [0.08575654 0.07897899 0.05976353 0.12574661 0.09795925 0.09357688\n",
      " 0.06172851 0.0998341  0.25457183 0.04208375]\n",
      "################################################################################\n",
      "i-th element: 27\n",
      "y: [1] [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "--------------------------------------------------------------------------------\n",
      "results after 100 iterations; argmax and prob. distr for softmax:\n",
      "--------------------------------------------------------------------------------\n",
      "1 [0.05803718 0.43899825 0.04232915 0.07799821 0.06466093 0.06234449\n",
      " 0.04359244 0.06563946 0.11590185 0.03049806]\n",
      "################################################################################\n",
      "i-th element: 28\n",
      "y: [2] [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      "--------------------------------------------------------------------------------\n",
      "results after 100 iterations; argmax and prob. distr for softmax:\n",
      "--------------------------------------------------------------------------------\n",
      "2 [0.06971468 0.19451661 0.20892851 0.08874842 0.07635588 0.07406499\n",
      " 0.05415282 0.07730456 0.11736543 0.0388481 ]\n",
      "################################################################################\n",
      "i-th element: 29\n",
      "y: [1] [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "--------------------------------------------------------------------------------\n",
      "results after 100 iterations; argmax and prob. distr for softmax:\n",
      "--------------------------------------------------------------------------------\n",
      "1 [0.02826185 0.69282493 0.06861111 0.03530026 0.03076443 0.02991541\n",
      " 0.02220201 0.03112292 0.04497352 0.01602356]\n",
      "################################################################################\n",
      "i-th element: 30\n",
      "y: [2] [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      "--------------------------------------------------------------------------------\n",
      "results after 100 iterations; argmax and prob. distr for softmax:\n",
      "--------------------------------------------------------------------------------\n",
      "2 [0.02921412 0.15940486 0.59595752 0.03601411 0.03166267 0.03083701\n",
      " 0.02316425 0.03201043 0.04489522 0.0168398 ]\n",
      "################################################################################\n",
      "i-th element: 31\n",
      "y: [5] [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      "--------------------------------------------------------------------------------\n",
      "results after 100 iterations; argmax and prob. distr for softmax:\n",
      "--------------------------------------------------------------------------------\n",
      "2 [0.05364627 0.17460273 0.23327792 0.06477189 0.05772486 0.20412058\n",
      " 0.04324807 0.058292   0.07837567 0.03194   ]\n",
      "################################################################################\n",
      "i-th element: 32\n",
      "y: [5] [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      "--------------------------------------------------------------------------------\n",
      "results after 100 iterations; argmax and prob. distr for softmax:\n",
      "--------------------------------------------------------------------------------\n",
      "5 [0.02209396 0.06116288 0.0734921  0.02647999 0.02371232 0.70633335\n",
      " 0.01789401 0.0239408  0.03165106 0.01323954]\n",
      "################################################################################\n",
      "i-th element: 33\n",
      "y: [7] [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "--------------------------------------------------------------------------------\n",
      "results after 100 iterations; argmax and prob. distr for softmax:\n",
      "--------------------------------------------------------------------------------\n",
      "5 [0.05150864 0.12046844 0.13615184 0.0607708  0.05497526 0.25856369\n",
      " 0.0423048  0.17227039 0.07122518 0.03176096]\n",
      "################################################################################\n",
      "i-th element: 34\n",
      "y: [6] [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "--------------------------------------------------------------------------------\n",
      "results after 100 iterations; argmax and prob. distr for softmax:\n",
      "--------------------------------------------------------------------------------\n",
      "5 [0.05757055 0.11305162 0.12213037 0.06662457 0.06101822 0.16547058\n",
      " 0.16195918 0.13921533 0.07624416 0.03671542]\n",
      "################################################################################\n",
      "i-th element: 35\n",
      "y: [5] [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      "--------------------------------------------------------------------------------\n",
      "results after 100 iterations; argmax and prob. distr for softmax:\n",
      "--------------------------------------------------------------------------------\n",
      "5 [0.02460533 0.04529797 0.04824723 0.02827126 0.02600841 0.666396\n",
      " 0.05978896 0.05348552 0.03204917 0.01585015]\n",
      "################################################################################\n",
      "i-th element: 36\n",
      "y: [3] [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      "--------------------------------------------------------------------------------\n",
      "results after 100 iterations; argmax and prob. distr for softmax:\n",
      "--------------------------------------------------------------------------------\n",
      "5 [0.04890345 0.08388029 0.08833068 0.19107795 0.0514812  0.24157878\n",
      " 0.10438656 0.09586747 0.06223172 0.0322619 ]\n",
      "################################################################################\n",
      "i-th element: 37\n",
      "y: [0] [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "--------------------------------------------------------------------------------\n",
      "results after 100 iterations; argmax and prob. distr for softmax:\n",
      "--------------------------------------------------------------------------------\n",
      "0 [0.20892749 0.08090999 0.08427252 0.13762453 0.05326387 0.15225911\n",
      " 0.0955596  0.08973367 0.06304609 0.03440313]\n",
      "################################################################################\n",
      "i-th element: 38\n",
      "y: [6] [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "--------------------------------------------------------------------------------\n",
      "results after 100 iterations; argmax and prob. distr for softmax:\n",
      "--------------------------------------------------------------------------------\n",
      "6 [0.0919285  0.04683659 0.04850074 0.07119765 0.03206075 0.07624532\n",
      " 0.52351956 0.05115871 0.03749671 0.02105547]\n",
      "################################################################################\n",
      "i-th element: 39\n",
      "y: [3] [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      "--------------------------------------------------------------------------------\n",
      "results after 100 iterations; argmax and prob. distr for softmax:\n",
      "--------------------------------------------------------------------------------\n",
      "3 [0.0706562  0.0400517  0.04134596 0.51639751 0.02805673 0.06109568\n",
      " 0.14779596 0.0433933  0.03256763 0.01863933]\n",
      "################################################################################\n",
      "i-th element: 40\n",
      "y: [1] [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "--------------------------------------------------------------------------------\n",
      "results after 100 iterations; argmax and prob. distr for softmax:\n",
      "--------------------------------------------------------------------------------\n",
      "1 [0.09039482 0.24851341 0.05768978 0.20463555 0.04035991 0.08058269\n",
      " 0.14392364 0.06021999 0.04639546 0.02728476]\n",
      "################################################################################\n",
      "i-th element: 41\n",
      "y: [0] [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "--------------------------------------------------------------------------------\n",
      "results after 100 iterations; argmax and prob. distr for softmax:\n",
      "--------------------------------------------------------------------------------\n",
      "0 [0.52663214 0.10366895 0.03667668 0.0945814  0.02618123 0.04938669\n",
      " 0.07693641 0.03815539 0.02991723 0.01786387]\n",
      "################################################################################\n",
      "i-th element: 42\n",
      "y: [2] [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      "--------------------------------------------------------------------------------\n",
      "results after 100 iterations; argmax and prob. distr for softmax:\n",
      "--------------------------------------------------------------------------------\n",
      "0 [0.21170606 0.12297325 0.21055178 0.11596225 0.03956821 0.07042913\n",
      " 0.10043497 0.05609568 0.04485054 0.02742811]\n",
      "################################################################################\n",
      "i-th element: 43\n",
      "y: [1] [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "results after 100 iterations; argmax and prob. distr for softmax:\n",
      "--------------------------------------------------------------------------------\n",
      "1 [0.08029935 0.61775201 0.08005511 0.05359976 0.02024467 0.03501619\n",
      " 0.04772203 0.02834188 0.02287589 0.01409312]\n",
      "################################################################################\n",
      "i-th element: 44\n",
      "y: [0] [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "--------------------------------------------------------------------------------\n",
      "results after 100 iterations; argmax and prob. distr for softmax:\n",
      "--------------------------------------------------------------------------------\n",
      "0 [0.61080359 0.14289331 0.0609279  0.04358187 0.0173293  0.02947057\n",
      " 0.03927877 0.02406828 0.01953926 0.01210716]\n",
      "################################################################################\n",
      "i-th element: 45\n",
      "y: [0] [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "--------------------------------------------------------------------------------\n",
      "results after 100 iterations; argmax and prob. distr for softmax:\n",
      "--------------------------------------------------------------------------------\n",
      "0 [0.81625548 0.06268453 0.02967444 0.02145449 0.00853829 0.01455899\n",
      " 0.01936574 0.01188549 0.00963782 0.00594474]\n",
      "################################################################################\n",
      "i-th element: 46\n",
      "y: [2] [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      "--------------------------------------------------------------------------------\n",
      "results after 100 iterations; argmax and prob. distr for softmax:\n",
      "--------------------------------------------------------------------------------\n",
      "2 [0.2468845  0.12346225 0.39970076 0.05268876 0.02202353 0.03673827\n",
      " 0.04797498 0.03028778 0.02475764 0.01548154]\n",
      "################################################################################\n",
      "i-th element: 47\n",
      "y: [3] [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      "--------------------------------------------------------------------------------\n",
      "results after 100 iterations; argmax and prob. distr for softmax:\n",
      "--------------------------------------------------------------------------------\n",
      "3 [0.14906571 0.1087758  0.16875702 0.37256725 0.0254502  0.04143842\n",
      " 0.05284964 0.03456356 0.02849831 0.0180341 ]\n",
      "################################################################################\n",
      "i-th element: 48\n",
      "y: [6] [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "--------------------------------------------------------------------------------\n",
      "results after 100 iterations; argmax and prob. distr for softmax:\n",
      "--------------------------------------------------------------------------------\n",
      "6 [0.1190813  0.09831829 0.12694876 0.16690985 0.02808977 0.04458108\n",
      " 0.32703312 0.03763719 0.03132205 0.02007858]\n",
      "################################################################################\n",
      "i-th element: 49\n",
      "y: [8] [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      "--------------------------------------------------------------------------------\n",
      "results after 100 iterations; argmax and prob. distr for softmax:\n",
      "--------------------------------------------------------------------------------\n",
      "6 [0.12692801 0.11187568 0.13196489 0.15284148 0.03825386 0.05876945\n",
      " 0.19408086 0.05034888 0.107241   0.02769588]\n"
     ]
    }
   ],
   "source": [
    "### training\n",
    "\n",
    "init_distribution = (-0.01, 0.01)\n",
    "layers_specs = [(20, act_fn), \n",
    "                (25, act_fn), \n",
    "                (30, act_fn), \n",
    "                (10, fn_softmax),\n",
    "                ]\n",
    "\n",
    "nn = NeuralNetMultiLayer(layers_specs=layers_specs, dim_input=64)\n",
    "\n",
    "\n",
    "\n",
    "for i in range(50):\n",
    "    print(\"#\" * 80)\n",
    "    x = X_train[i]\n",
    "    y_target = y_train[i]\n",
    "\n",
    "    print(\"i-th element:\", i)\n",
    "    print(\"y:\", np.where(y_target)[0], y_target)\n",
    "#     print(\"x:\", x)\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    lst_max_prob = []\n",
    "    for t in range(100):\n",
    "        nn.backpropagate(x, y_target, eta=0.01) \n",
    "        lst_x, lst_z, lst_y = nn.forward(x)\n",
    "        out = lst_y[-1]\n",
    "        ind_opt = np.argmax(out)\n",
    "        # this is only to store probs on 4 decimals \n",
    "        # nothing fancy but just for printing since \n",
    "        # we print so many lines, etc. \n",
    "        prob = round(10000 * out[ind_opt]) / 10000 \n",
    "        lst_max_prob.append((np.argmax(out), prob))\n",
    "    \n",
    "#     print(lst_max_prob)\n",
    "    print(\"results after 100 iterations; argmax and prob. distr for softmax:\")\n",
    "    print(\"-\" * 80)\n",
    "    print(ind_opt, out)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### real keras code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/milan/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /home/milan/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/20\n",
      "1000/1000 [==============================] - 0s 102us/step - loss: 2.4114 - accuracy: 0.1050\n",
      "Epoch 2/20\n",
      "1000/1000 [==============================] - 0s 9us/step - loss: 2.3746 - accuracy: 0.1160\n",
      "Epoch 3/20\n",
      "1000/1000 [==============================] - 0s 9us/step - loss: 2.3386 - accuracy: 0.1070\n",
      "Epoch 4/20\n",
      "1000/1000 [==============================] - 0s 9us/step - loss: 2.3438 - accuracy: 0.0890\n",
      "Epoch 5/20\n",
      "1000/1000 [==============================] - 0s 10us/step - loss: 2.3222 - accuracy: 0.1130\n",
      "Epoch 6/20\n",
      "1000/1000 [==============================] - 0s 19us/step - loss: 2.3156 - accuracy: 0.1140\n",
      "Epoch 7/20\n",
      "1000/1000 [==============================] - 0s 12us/step - loss: 2.2976 - accuracy: 0.1300\n",
      "Epoch 8/20\n",
      "1000/1000 [==============================] - 0s 12us/step - loss: 2.3047 - accuracy: 0.1100\n",
      "Epoch 9/20\n",
      "1000/1000 [==============================] - 0s 10us/step - loss: 2.3102 - accuracy: 0.1030\n",
      "Epoch 10/20\n",
      "1000/1000 [==============================] - 0s 11us/step - loss: 2.3059 - accuracy: 0.1240\n",
      "Epoch 11/20\n",
      "1000/1000 [==============================] - 0s 17us/step - loss: 2.3097 - accuracy: 0.1060\n",
      "Epoch 12/20\n",
      "1000/1000 [==============================] - 0s 15us/step - loss: 2.3004 - accuracy: 0.1080\n",
      "Epoch 13/20\n",
      "1000/1000 [==============================] - 0s 12us/step - loss: 2.3020 - accuracy: 0.1110\n",
      "Epoch 14/20\n",
      "1000/1000 [==============================] - 0s 12us/step - loss: 2.3011 - accuracy: 0.1130\n",
      "Epoch 15/20\n",
      "1000/1000 [==============================] - 0s 11us/step - loss: 2.3007 - accuracy: 0.1120\n",
      "Epoch 16/20\n",
      "1000/1000 [==============================] - 0s 15us/step - loss: 2.3017 - accuracy: 0.1290\n",
      "Epoch 17/20\n",
      "1000/1000 [==============================] - 0s 16us/step - loss: 2.2967 - accuracy: 0.1060\n",
      "Epoch 18/20\n",
      "1000/1000 [==============================] - 0s 12us/step - loss: 2.2997 - accuracy: 0.1140\n",
      "Epoch 19/20\n",
      "1000/1000 [==============================] - 0s 11us/step - loss: 2.3050 - accuracy: 0.1080\n",
      "Epoch 20/20\n",
      "1000/1000 [==============================] - 0s 10us/step - loss: 2.3028 - accuracy: 0.1220\n",
      "100/100 [==============================] - 0s 383us/step\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "# Generate dummy data\n",
    "import numpy as np\n",
    "x_train = np.random.random((1000, 20))\n",
    "y_train = keras.utils.to_categorical(np.random.randint(10, size=(1000, 1)), num_classes=10)\n",
    "x_test = np.random.random((100, 20))\n",
    "y_test = keras.utils.to_categorical(np.random.randint(10, size=(100, 1)), num_classes=10)\n",
    "\n",
    "model = Sequential()\n",
    "# Dense(64) is a fully-connected layer with 64 hidden units.\n",
    "# in the first layer, you must specify the expected input data shape:\n",
    "# here, 20-dimensional vectors.\n",
    "model.add(Dense(64, activation='relu', input_dim=20))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=sgd,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train, y_train,\n",
    "          epochs=20,\n",
    "          batch_size=128)\n",
    "score = model.evaluate(x_test, y_test, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1000/1000 [==============================] - 0s 138us/step - loss: 0.7226 - accuracy: 0.4920\n",
      "Epoch 2/20\n",
      "1000/1000 [==============================] - 0s 12us/step - loss: 0.7121 - accuracy: 0.5030\n",
      "Epoch 3/20\n",
      "1000/1000 [==============================] - 0s 15us/step - loss: 0.6996 - accuracy: 0.5080\n",
      "Epoch 4/20\n",
      "1000/1000 [==============================] - 0s 16us/step - loss: 0.7056 - accuracy: 0.4930\n",
      "Epoch 5/20\n",
      "1000/1000 [==============================] - 0s 18us/step - loss: 0.7027 - accuracy: 0.5020\n",
      "Epoch 6/20\n",
      "1000/1000 [==============================] - 0s 15us/step - loss: 0.7030 - accuracy: 0.5010\n",
      "Epoch 7/20\n",
      "1000/1000 [==============================] - 0s 15us/step - loss: 0.7064 - accuracy: 0.4870\n",
      "Epoch 8/20\n",
      "1000/1000 [==============================] - 0s 20us/step - loss: 0.6937 - accuracy: 0.5230\n",
      "Epoch 9/20\n",
      "1000/1000 [==============================] - 0s 20us/step - loss: 0.6959 - accuracy: 0.5190\n",
      "Epoch 10/20\n",
      "1000/1000 [==============================] - 0s 17us/step - loss: 0.6990 - accuracy: 0.5010\n",
      "Epoch 11/20\n",
      "1000/1000 [==============================] - 0s 16us/step - loss: 0.6963 - accuracy: 0.5000\n",
      "Epoch 12/20\n",
      "1000/1000 [==============================] - 0s 16us/step - loss: 0.6971 - accuracy: 0.4980\n",
      "Epoch 13/20\n",
      "1000/1000 [==============================] - 0s 17us/step - loss: 0.6936 - accuracy: 0.5200\n",
      "Epoch 14/20\n",
      "1000/1000 [==============================] - 0s 13us/step - loss: 0.6878 - accuracy: 0.5330\n",
      "Epoch 15/20\n",
      "1000/1000 [==============================] - 0s 11us/step - loss: 0.6942 - accuracy: 0.4990\n",
      "Epoch 16/20\n",
      "1000/1000 [==============================] - 0s 11us/step - loss: 0.6922 - accuracy: 0.5230\n",
      "Epoch 17/20\n",
      "1000/1000 [==============================] - 0s 11us/step - loss: 0.6927 - accuracy: 0.5290\n",
      "Epoch 18/20\n",
      "1000/1000 [==============================] - 0s 18us/step - loss: 0.6915 - accuracy: 0.5290\n",
      "Epoch 19/20\n",
      "1000/1000 [==============================] - 0s 13us/step - loss: 0.6942 - accuracy: 0.5090\n",
      "Epoch 20/20\n",
      "1000/1000 [==============================] - 0s 11us/step - loss: 0.6897 - accuracy: 0.5270\n",
      "100/100 [==============================] - 0s 672us/step\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "\n",
    "# Generate dummy data\n",
    "x_train = np.random.random((1000, 20))\n",
    "y_train = np.random.randint(2, size=(1000, 1))\n",
    "x_test = np.random.random((100, 20))\n",
    "y_test = np.random.randint(2, size=(100, 1))\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(64, input_dim=20, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train, y_train,\n",
    "          epochs=20,\n",
    "          batch_size=128)\n",
    "score = model.evaluate(x_test, y_test, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_hidden = 10\n",
    "model = NeuralNet(n_features, n_hidden, n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.loss(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.accuracy(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_prediction(model, sample_idx=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses, accuracies, accuracies_test = [], [], []\n",
    "losses.append(model.loss(X_train, y_train))\n",
    "accuracies.append(model.accuracy(X_train, y_train))\n",
    "accuracies_test.append(model.accuracy(X_test, y_test))\n",
    "\n",
    "print(\"Random init: train loss: %0.5f, train acc: %0.3f, test acc: %0.3f\"\n",
    "      % (losses[-1], accuracies[-1], accuracies_test[-1]))\n",
    "\n",
    "for epoch in range(15):\n",
    "    for i, (x, y) in enumerate(zip(X_train, y_train)):\n",
    "        model.train(x, y, 0.1)\n",
    "\n",
    "    losses.append(model.loss(X_train, y_train))\n",
    "    accuracies.append(model.accuracy(X_train, y_train))\n",
    "    accuracies_test.append(model.accuracy(X_test, y_test))\n",
    "    print(\"Epoch #%d, train loss: %0.5f, train acc: %0.3f, test acc: %0.3f\"\n",
    "          % (epoch + 1, losses[-1], accuracies[-1], accuracies_test[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(losses)\n",
    "plt.title(\"Training loss\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(accuracies, label='train')\n",
    "plt.plot(accuracies_test, label='test')\n",
    "plt.ylim(0, 1.1)\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.legend(loc='best');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_prediction(model, sample_idx=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## c) Exercises\n",
    "\n",
    "### Look at worst prediction errors\n",
    "\n",
    "- Use numpy to find test samples for which the model made the worst predictions,\n",
    "- Use the `plot_prediction` to look at the model predictions on those,\n",
    "- Would you have done any better?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/worst_predictions.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyper parameters settings\n",
    "\n",
    "- Experiment with different hyper parameters:\n",
    "  - learning rate,\n",
    "  - size of hidden layer,\n",
    "  - initialization scheme: test with 0 initialization vs uniform,\n",
    "  - implement other activation functions,\n",
    "  - implement the support for a second hidden layer.\n",
    "\n",
    "\n",
    "### Mini-batches\n",
    "\n",
    "- The current implementations of `train` and `grad_loss` function currently only accept a single sample at a time:\n",
    "    - implement the support for training with a mini-batch of 32 samples at a time instead of one,\n",
    "    - experiment with different sizes of batches,\n",
    "    - monitor the norm of the average gradients on the full training set at the end of each epoch.\n",
    "\n",
    "\n",
    "### Momentum\n",
    "\n",
    "- Bonus: Implement momentum\n",
    "\n",
    "\n",
    "### Back to Keras\n",
    "\n",
    "- Implement the same network architecture with Keras;\n",
    "\n",
    "- Check that the Keras model can approximately reproduce the behavior of the Numpy model when using similar hyperparameter values (size of the model, type of activations, learning rate value and use of momentum);\n",
    "\n",
    "- Compute the negative log likelihood of a sample 42 in the test set (can use `model.predict_proba`);\n",
    "\n",
    "- Compute the average negative log-likelihood on the full test set.\n",
    "\n",
    "- Compute the average negative log-likelihood  on the full training set and check that you can get the value of the loss reported by Keras.\n",
    "\n",
    "- Is the model overfitting or underfitting? (ensure that the model has fully converged by increasing the number of epochs to 50 or more if necessary)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/keras_model.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/keras_model_test_loss.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homework assignments\n",
    "\n",
    "- Watch the following video on [how to code a minimal deep learning framework](https://www.youtube.com/watch?v=o64FV-ez6Gw) that feels like a simplified version\n",
    "of Keras but using numpy instead of tensorflow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import YouTubeVideo\n",
    "YouTubeVideo(\"o64FV-ez6Gw\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Optional**: read the following blog post on Reverse-Mode Automatic Differentiation from start to section \"A simple implementation in Python\" included:\n",
    "\n",
    "  https://rufflewind.com/2016-12-30/reverse-mode-automatic-differentiation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
